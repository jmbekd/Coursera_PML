{
    "contents" : "---\ntitle: \"Coursera PML Assignment\"\nauthor: \"John Montgomery-Brown\"\ndate: \"Friday, October 24, 2014\"\noutput:\n  html_document:\n    keep_md: yes\n---\n\nHuman Activity Recognition - Predicting The Quality of Execution\n================================================================\n\n## Synopsis\nA Random Forest model was created to predict how well an individual executed \n(i.e., the quality of execution) a bicep curl using a dumbbell. The out-of-\nbag error was estimated to be 0.97% based on 10-fold cross-validation and the \nout-of-sample error rate for the validation data set was approximately 0.79%;\nthese results suggested that the Random Forest model was very accurate. The \nRandom Forest model was applied to the test data set and all twenty predictions\ngenerated by the model were correct (i.e., for the test data set, the model was\n100% accurate).\n\n## Introduction\n\nWhile most research on human activity recognition has focused on differentiating\nbetween various activities, little research has been conducted on determining \nhow well an activity was conducted. In their 2013 paper, Velloso et al. attempted\nto determine how well an individual executed a weight lifing exercise (namely, \ndumbbell curl).\n\nFor this assignment, I used data developed by Velloso et al. (2013) to develop a\nmodel that could be used to predict the quality of execution for a dumbbell curl.\n[R 3.1.1](http://www.r-project.org/) and [RStudio 0.98.1074](http://www.rstudio.com/products/RStudio/) \nwere used for this analysis.\n\n## Downloading the Data\n\nData for this model were obtained from the URL and file names listed in the R \ncode below. These data are based on the [Weight Lifting Exercises Dataset](http://groupware.les.inf.puc-rio.br/har) of Velloso et al. (2013)\n; the original data set is available [here](http://groupware.les.inf.puc-rio.br/static/WLE/WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv). \n\nThe code below was used to set up the data directory, load any required packages, \nto download the data and save it as a 'rda' file for faster loading during \nsubsequent \"knitting\" operations.\n\n```{r results = \"hide\"}\n## Set up the data directory and load required packages\ndir <- \"./data/\"\npkgs <- c(\"data.table\", \"caret\", \"randomForest\", \"e1071\")\n# in the code below suppressMessages was used to prevent the output from stating\n# \"Loading required package: x\" where x is the name of a required package\nsuppressMessages(sapply(pkgs, function(x) { \n  if (!require(x, character.only = TRUE)) \n    {install.packages(x, repos = \"http://cran.r-project.org\", dependencies = TRUE)}\n  require(x, character.only = TRUE)\n  } ) )\n\n## Download data if it hasn't been downloaded or if it has, load it from a saved version\n# to prevent knitr from throwing an error when downloading the files, I dropped\n# the 's' from https:// as suggested here:\n# http://stackoverflow.com/questions/19890633/r-produces-unsupported-url-scheme-error-when-getting-data-from-https-sites\nURL <- \"http://d396qusza40orc.cloudfront.net/predmachlearn/\" \nnames <- c(\"pml-training.csv\", \"pml-testing.csv\")\nif (!(\"pml_data.rda\" %in% list.files(dir))) {\n  if (!file.exists(dir)) dir.create(dir)\n  dateDownloaded <- sapply(names, function(x) {\n    download.file(paste0(URL, x), destfile = paste0(dir, x), mode = \"wb\")\n    date()\n    } )\n  \n  train <- fread(paste0(dir, \"pml-training.csv\"))\n  test <- fread(paste0(dir, \"pml-testing.csv\"))\n  \n  save(train, test, dateDownloaded, file = paste0(dir, \"pml_data.rda\"))\n  } else {\n    load(paste0(dir, \"pml_data.rda\"))\n    }\n```\n\n## Data Processing\nOn examining the data, a large number of missing values were noted and a number \nof columns contained \"DIV/0!\" values. The entries with missing values were all\ncontained within the calculated summary columns (i.e., mean, variance, standard\ndeviation, max, min, amplitude, kurtosis, skewness) for the original data. In \naddition, \"DIV/0!\" values were only observed in these same columns (the \"DIV/0!\" \nvalues appear to have arisen whenever the standard deviation or variance, \nwithin a particular time window, were equal to zero).\n\nRather than working with calculated summary columns or imputing values for the \nmissing and or \"DIV/0!\" data, I opted to remove these columns from the data set \n(if I had chosen to work with these columns, I would also have had to recalculate \nall of the summary data to ensure that an incorrect formula was not applied to \nthe original data set. For example, during exploration of the data, it was noted \nthat there were skewness\\_roll\\_belt and skewness\\_roll\\_belt.1 columns and that \nthe skewness\\_pitch\\_belt column was missing. This error suggests that there may \nhave been other errors with the calculated data) The initial data processing was \nconducted using the code presented below.\n\n```{r}\n## Preprocessing\npreprocess2 <- function(DT) {\n  DT_sub <- DT[, 8:ncol(DT), with = FALSE] # the first 7 columns are informational\n  for (cols in seq_len(ncol(DT_sub) - 1)) # convert columns to numeric\n    set(DT_sub, j = cols, value = as.numeric(DT_sub[[cols]])) \n  # remove columns with NA valuse from the data set\n  DT_sub[, (which(is.na(colSums(DT_sub[, !\"classe\", with = FALSE])))) := NULL, \n         with = FALSE]\n  DT_sub\n  }\n\nsuppressWarnings(train_pp2 <- preprocess2(train))\n```\n\nHaving removed the calculated summary columns, there were 52 data columns (Euler \nangle (roll, pitch, and yaw) data, acceleration, gyroscope, and magnetometer \ndata (x, y, and z components for each of the acceleration, gyroscope, and \nmagnetometer data), and total acceleration data for the sensors located on the \nbelt, arm, forearm, and dumbbell; (3 + 3 * 3 + 1) * 4 = 52) and the classification\ncolumn (\"classe\").\n\n## Developing a Random Forest Model\n\nWith processed training data, I created a training and validation data sets. \nWith the training data set, I used the `caret` package to developed a random \nforest model for the centered and scaled data using 10-fold cross-validation. \nAs this step took a long time on my computer, I cached the results using the \n`cache = TRUE` option for the R code in `knitr`.\n\n```{r cache = TRUE}\nset.seed <- 45\npart <- as.vector(createDataPartition(train_pp2$classe, p = 0.6, list = FALSE))\n\ntrain_fit <- train(as.factor(classe) ~ ., data = train_pp2[part], method = \"rf\",\n                   trControl = trainControl(method = \"cv\"), metric = \"Accuracy\",\n                   preProcess = c(\"center\", \"scale\"))\ntrain_fit$finalModel\n```\n\nThe final Random Forest model had an out-of-bag (\"OOB\") error rate of approximately \n`r 100 * round(train_fit$finalModel$err.rate[train_fit$finalModel$ntree , 1], 4)`% and \n`r train_fit$finalModel$mtry` predictors were selected at random for the \nsplitting at each node. \n\n```{r}\nimp_params <- data.frame(\"parameter\" = rownames(train_fit$finalModel$importance), \"MeanDecreaseGini\" = train_fit$finalModel$importance)\nrownames(imp_params) <- NULL\nimp_params <- imp_params[order(-imp_params$MeanDecreaseGini), ]\ndotchart(rev(imp_params$MeanDecreaseGini[1:25]),\n         labels = rev(imp_params$parameter[1:25]),\n         cex = 0.7,\n         col = c(rep(\"black\", 15), rep(\"red\", 10)),\n         xlab = \"Average Decrease in Gini Coefficient\",\n         main = \"Top 25 Parameters in Final Model\")\n```\n\nIn the figure above, the ten most important parameters (as measured by the \naverage decrease in the Gini coefficient) in the final Random Forest model are \nplotted in red. Of the ten most important parameters (i.e., data columns), \nthree are located in the sensors located on the weightlifting belt, three in the\nsensors on the forearm, and four in the sensors on the dumbbell; of these sensors, \nthe sensor located on the belt appears to have the most predictive value. \nInterestingly, the type of data with the most predictive value appear to be Euler \nangle data (i.e., five of ten parameters with the highest importance are Euler \nangle data).\n\n## Testing the Random Forest Model Against the Validation Data Set\n\nAs the OOB error rate for the final Random Forest model is only an estimate of \nthe out-of-sample error rate, I used the validation data to determine \nthe out-of-sample error rate (i.e., to test the accuracy of the OOB error rate \nestimate). \n\n```{r}\nvalid_test <- confusionMatrix(data = predict(train_fit, \n                              newdata = train_pp2[!part, ][, !\"classe\", with = FALSE]),\n                              reference = train_pp2[!part]$classe)\nvalid_test\n```\n\nAs seen above, the validation data indicate that the out-of-sample error rate is \napproximately `r 100 * round(1 - valid_test$overall[1], 4)`%. This result suggests \nthat the OOB error rate is a good estimate of the out-of-sample error rate and that\nthe Random Forest model is very accurate. \n\n## Using the Random Forest Model to Predict the Quality of Execution for the Test Data Set\n\nDue to its low OOB and out-of-sample error rates, the final Random Forest model \nwas used on the test data (after preprocessing. During preprocessing, an error \nis thrown because the \"classe\" column is not present in the test data. On \nexamining the test data after preprocessing, it appears that this column was \nreplaced by a \"problem_id\" column with the numbers 1 through 20. As this column \nis not part of the data being used for prediction, it was removed during the \nprediction step) with the resulting predictions shown below. \n\n```{r}\ntest_pp2 <- preprocess2(test)\ntest_pred <- as.character(predict(train_fit, \n                                  newdata = test_pp2[, !\"problem_id\", with = FALSE]))\ntest_pred\n```\n\nThe predictions for the test data set were written to text files (one text file \nper prediction) and uploaded to the [Coursera Practical Machine Learning Assignment Submission webpage](https://class.coursera.org/predmachlearn-006/assignment). \nAll of the model's predictions for the test data set were correct.\n\n```{r}\npml_write_files <- function(x) {\n  n <- length(x)\n  for (i in 1:n) {\n    filename <- paste0(dir, \"problem_id_\", i, \".txt\")\n    write.table(x[i], file = filename, quote = FALSE, \n                row.names = FALSE, col.names = FALSE)\n  }\n}\npml_write_files(test_pred)\n```\n\n## Summary\n\nIn this analysis, I developed a Random Forest model that can be used to predict\nthe quality of execution of a dumbbell curl. The training data set was divided \ninto training and validation data sets and a Random Forest model was developed \nbased on the training data set. Based on a 10-fold cross-validation of the training\ndata set, the out-of-bag error was approximately `r 100 * round(train_fit$finalModel$err.rate[train_fit$finalModel$ntree , 1], 4)`%. \nTo confirm whether the out-of-bag error estimate was a good approximation of the\nout-of-sample error rate, I used the Random Forest model to predict the quality \nof execution for the validation data set; for the validation data set predictions, \nthe model had an out-of-sample error rate of approximately `r 100 * round(1 - valid_test$overall[1], 4)`%.\nThis result suggested that the model was very accurate and when the model was \napplied to the test data set, the resulting predictions were 100% accurate. \n\n## Reference\n\nVelloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. \n[Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf). \nProceedings of 4th International Conference in Cooperation with SIGCHI \n(Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.\n\nRead more [here](http://groupware.les.inf.puc-rio.br/har#ixzz3GwPd7jJc)\nand [here](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201).\n",
    "created" : 1414208726039.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "143887544",
    "id" : "40B3316F",
    "lastKnownWriteTime" : 1414212434,
    "path" : "C:/Users/emilyd/Desktop/Rfiles/Coursera_PML/pml_assignment.rmd",
    "project_path" : "pml_assignment.rmd",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}